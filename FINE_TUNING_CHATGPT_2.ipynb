{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import os\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model = GPT2ForSequenceClassification.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "# Define the training data and labels\n",
        "train_texts = [\n",
        "    \"Example sentence 1 for class 0\",\n",
        "    \"Example sentence 2 for class 1\",\n",
        "    # Add more sentences\n",
        "]\n",
        "train_labels = [0, 1]  # Replace with actual labels\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Preprocess the data and encode it into features\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "\n",
        "# Define tensors for input ids, attention masks, and labels\n",
        "train_input_ids = torch.tensor(train_encodings['input_ids'])\n",
        "train_attention_masks = torch.tensor(train_encodings['attention_mask'])\n",
        "train_labels = torch.tensor(train_labels)\n",
        "\n",
        "val_input_ids = torch.tensor(val_encodings['input_ids'])\n",
        "val_attention_masks = torch.tensor(val_encodings['attention_mask'])\n",
        "val_labels = torch.tensor(val_labels)\n",
        "\n",
        "# Create DataLoader for training and validation\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
        "\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Set up an optimizer and criterion\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Set up a learning rate scheduler\n",
        "total_steps = len(train_dataloader) * 5  # 5 epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# Set up Tensorboard logging\n",
        "log_dir = \"./logs\"\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "# Set up early stopping\n",
        "early_stopping = {'patience': 3, 'counter': 0, 'best_val_loss': float('inf')}\n",
        "\n",
        "# Set up model saving\n",
        "best_model_path = \"./best_model.pth\"\n",
        "\n",
        "# Set up a training loop with validation, early stopping, and model saving\n",
        "def train_model(model, train_dataloader, val_dataloader, optimizer, criterion, scheduler, device, num_epochs=5):\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for batch in train_dataloader:\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            input_ids, attention_masks, labels = batch\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Gradient accumulation (if needed)\n",
        "        # optimizer.step()  # Move this line outside of the loop if not using gradient accumulation\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "        print(f\"Train Loss: {avg_train_loss}\")\n",
        "        writer.add_scalar(\"Train Loss\", avg_train_loss, epoch)\n",
        "\n",
        "        # Perform optimization step after accumulating gradients (if needed)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Learning rate scheduler step\n",
        "        scheduler.step()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        with torch.no_grad():\n",
        "            for batch in val_dataloader:\n",
        "                batch = tuple(t.to(device) for t in batch)\n",
        "                input_ids, attention_masks, labels = batch\n",
        "\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, preds = torch.max(outputs.logits, 1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_dataloader)\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        print(f\"Validation Loss: {avg_val_loss}, Accuracy: {accuracy * 100:.2f}%\")\n",
        "        writer.add_scalar(\"Validation Loss\", avg_val_loss, epoch)\n",
        "        writer.add_scalar(\"Accuracy\", accuracy, epoch)\n",
        "\n",
        "        # Early stopping\n",
        "        if avg_val_loss < early_stopping['best_val_loss']:\n",
        "            early_stopping['best_val_loss'] = avg_val_loss\n",
        "            early_stopping['counter'] = 0\n",
        "            # Save the best model\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "        else:\n",
        "            early_stopping['counter'] += 1\n",
        "            if early_stopping['counter'] >= early_stopping['patience']:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_dataloader, val_dataloader, optimizer, criterion, scheduler, device, num_epochs=4)\n",
        "\n",
        "# Load the best model\n",
        "best_model = GPT2ForSequenceClassification.from_pretrained('gpt2')\n",
        "best_model.load_state_dict(torch.load(best_model_path))\n",
        "best_model.to(device)\n",
        "\n",
        "# Now you can use 'best_model' for inference."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hifiz3IldBg",
        "outputId": "b6223b65-574c-48ef-acc9-b06f26acb163"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "Train Loss: 0.0002840353990904987\n",
            "Validation Loss: 9.389070510864258, Accuracy: 0.00%\n",
            "Epoch 2/4\n",
            "Train Loss: 0.0029093578923493624\n",
            "Validation Loss: 9.83505630493164, Accuracy: 0.00%\n",
            "Epoch 3/4\n",
            "Train Loss: 6.317892984952778e-05\n",
            "Validation Loss: 10.1011962890625, Accuracy: 0.00%\n",
            "Epoch 4/4\n",
            "Train Loss: 4.076874756719917e-05\n",
            "Validation Loss: 10.249918937683105, Accuracy: 0.00%\n",
            "Early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2ForSequenceClassification(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}